<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0031)http://www.cs.cmu.edu/~aayushb/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Stolen from Sergey */
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }
  </style>
  <link rel="icon" type="image/jpg" href="https://www.ri.cmu.edu/wp-content/uploads/2017/01/RI_no-text_small.jpg">

  <title>Minh P. Vo</title>

  <link href="./index_files/myStyleX.css" rel="stylesheet" type="text/css">
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td width="67%" valign="middle">
                <p align="center">
                  <font size="6">Minh Phuoc Vo</font>
                </p>
                <p>

                  <b>
                    <font size="3">Head of Engineering
                  </b>
                  <br><a href="spreeai.com"> SpreeAI</a>
                  <br><br>Email: [middle][first][last] at [gmail dot com
                  <!-- <br><a href="https://minhpvo.github.io/CV.pdf"> CV</a> (updated in 11-2022)  -->
                  <br><a href="https://scholar.google.com/citations?user=H4g_IKkAAAAJ&hl=en"> Google Scholar</a>
                </p>
                <p>
              <td width="25%"><img width="300" src="./index_files/minh3.jpg"></td>
            </tr>
          </tbody>
        </table>



        <!-- I am a research scientist at Facebook Reality Lab. Previously, I was a Ph.D. student at The Robotics Institute, Carnegie Mellon University where I worked with <a href="http://www.cs.cmu.edu/~srinivas"><b>Prof. Srinivasa Narasimhan</b></a> and <a href="http://www.cs.cmu.edu/~yaser"><b>Prof. Yaser Sheikh</b></a> on novel methods to capture dense and accurate 3D shape of human bodies. -->
        <!-- Before that, I worked with <a href="http://faculty.cua.edu/wangz/"><b>Prof. Zhaoyang Wang</b></a> at the Catholic University of America, where I got B.E. degree in Electrical Engineering, on camera calibration, structured light system, and tracking algorithms. <br>&nbsp; -->
        <p><b><font color="blue"> New: </font></b>I am co-organizing <a href="https://aayushbansal.xyz/3q-vton/">Three questions about virtual try-on @ CVPR 2025</a> with <a href="https://aayushbansal.xyz">Aayush Bansal</a>.</p>

        
        I am the Head of Engineering at <a href="https://spreeai.com/">SpreeAI</a>, a high-tech virtual try-on
        startup. Besides overseeing all products R&D, I also lead and grow a world-class team of passionate Machine Learning researchers and engineers to develop and productionize
        our photorealistic avatar technology.
        Previously, I was a Research Scientist at <a href=https://about.meta.com/realitylabs/">Meta Reality Labs
          Research</a>, where I tech-led a group of researchers to develop 3D perception and human sensing algorithms
        for <a href="https://about.meta.com/realitylabs/projectaria/">Meta Aria glasses</a>.
        Before that, I was a Ph.D. student at The Robotics Institute, Carnegie Mellon University where I worked with <a
          href="http://www.cs.cmu.edu/~srinivas">Prof. Srinivasa Narasimhan</a> and <a
          href="http://www.cs.cmu.edu/~yaser">Prof. Yaser Sheikh</a> on novel methods to capture dense and accurate 3D
        shape of human bodies.
        I also worked with <a href="http://faculty.cua.edu/wangz/">Prof. Zhaoyang Wang</a> at the Catholic University of
        America, where I got B.E. degree in Electrical Engineering, on camera calibration, structured light system, and
        tracking algorithms. <br>&nbsp;

       
        </b>&nbsp;  <br><b></b>I am also working part-time with a consulting company, <a href="https://lecon.dev/">LeCON LLC</a>, to help startups and companies to further develop capabilities in large vision language model (VLM), including data curation, model training, and deployment, and various core components for intelligent systems such as mapping, perception, and prediction. Reach out to <a href="mailto:team@lecon.dev">us</a> if you are interested in collaborating. <br>&nbsp; 
        <br>

        <h2>Research</h2>
        <p>I am very interested in various aspects of 3D vision, physics-based vision, and generative models for
          photorealistic digitial avatar creation and human scene understanding. The goal is to develop holistic and
          end-to-end machine learning systems that understand and recreate virtual environments that are perceptually
          indistinguishable from reality.</p>

        <!-- <p><b>Internships</b>: I am always looking for strong graduate students to collaborate with. If you are interested in interning at FRL with me, send me an email detailing what your research interests are, and what you would like to work on during the internship.</p> -->
        <p><b>Jobs opportuninty</b>: I am hiring full time CV&ML&Graphics researchers. I strike to balance between core
          and applied research with patents, papers, and product as outputs. Send me an email if you are interested in
          working with me.</p>

    
        <h2>Award</h2>
        <ul>
          <li>
            <p> <b> Jun. 2024</b> Ego4D and EgoHuman are selected as <a
                href=https://egovis.github.io/awards/2022_2023/> <b>2022-2023 distingished egocentric papers </b></a>at
                CVPR's EgoVision workshop.</p>
          </li>
          <li>
            <p> <b> Jun. 2022</b> Ego4D is included in <a
                href=https://twitter.com/cvpr/status/1539772091112857600?lang=en> <b>33 Best Paper Finalist at
                  CVPR</b></a>.</p>
          </li>
          <li>
            <p> <b> May. 2017</b> Won the <a
                href=https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship> <b>Qualcomm
                  Innovation Fellowship </b></a> with <a href=http://www.cs.cmu.edu/~aayushb /> <b>Aayush
                Bansal</b></a>. See <a
                href="https://www.scs.cmu.edu/news/two-student-teams-win-qualcomm-innovation-fellowships"> here</a> for
              the press coverage.</p>
          </li>
          <li>
            <p> <b> Jan. 2014</b> Our "<a
                href="http://iopscience.iop.org/article/10.1088/0957-0233/25/3/035401/meta"><b>Stereo
                  Reconstruction</b></a>" paper (with my previous group) has been awarded <a
                href="http://cms.iopscience.iop.org/alfresco/d/d/workspace/SpacesStore/3d1ba11a-1409-11e5-b821-29411a5deefe/L%20MST%200515%20highlights-A4-2.pdf"><b>Outstanding
                  Paper</b></a> from Measurement Science and Technology. </p>
          </li>
        </ul>

        <h2>Patent</h2>
        <ul>
          <li>
            <p> <b> Mar. 2022</b> <a
                href=https://patentimages.storage.googleapis.com/6e/0c/00/54e3fd04a1a2f1/US20220082679A1.pdf>
                <b>Time-synchronized Distributed Passive Captures </b></a>.</p>
          </li>
          <li>
            <p> <b> Feb. 2022</b> <a
                href=https://patentimages.storage.googleapis.com/0d/4a/91/6b49db1091a675/US20220036626A1.pdf>
                <b>Learning a realistic and animatable full body human avatar from monocular video </b></a>.</p>
          </li>
          <li>
            <p> <b> Jan. 2020</b> <a
                href=https://patentimages.storage.googleapis.com/ab/92/77/3386faab98dcd8/US10535156.pdf> <b>Scene
                  reconstruction from bursts of image data </b></a>.</p>
          </li>
        </ul>

        <h2>Publication</h2>
        <table cellspacing="15">

          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/effiActionReg.png" border="0">
            </td>
            <td>
              <p> <b>Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling</b>
                <br>Khoi-Nguyen Mac, Minh Do, <b>Minh Vo</b><br>
                <em>IEEE Trans. Image Process. 2023 </em> &nbsp; <b></b>
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10236596">PDF</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 280px;" src="./index_files/Images/egoHuman.png" border="0">
            </td>
            <td>
              <p> <b>EgoHumans: An Egocentric 3D Multi-Human Benchmark</b>
                <br>Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, <b>Minh Vo</b>, Kris Kitani<br>
                <em>ICCV 2023 </em><b> <font color="blue">(Oral and distingished egocentric papers)  
                </b>&nbsp;<b></b>Acceptance ratio: 152/8260 = 1.8%
                <br> <a href="https://rawalkhirodkar.github.io/egohumans/files/paper.pdf">PDF</a> <a
                  href="https://rawalkhirodkar.github.io/">Project Page</a>
              </p>
            </td>
          </tr>


          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/Snipper.gif" border="0">
            </td>
            <td>
              <p> <b>Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and
                  Forecasting on a Video Snippet</b>
                <br>Shihao Zou, Yuanlu Xu, Chao Li, Lingni Ma, Li Chen, <b>Minh Vo</b><br>
                <em>IEEE Trans. on Circuits and Systems for Video Technology, 2023 </em> &nbsp; <b></b>
                <br>
                <a href="https://arxiv.org/abs/2207.04320">PDF</a> <a href="https://github.com/JimmyZou/Snipper">Project
                  Page</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/ideo.png" border="0">
            </td>
            <td>
              <p> <b>IDEO: Large Scale Egocentric 3D Object Dataset and Benchmark Challenges </b>
                <br>Tien Do, Lance Lemke, Jingfan Guo, Khiem Vuong, <b>Minh Vo</b>, Hyun Soo Park <br>
                <em>arxiv 2022 </em> &nbsp; <b></b>
                <br>
                <a href="https://openreview.net/pdf?id=C_QwhQq4r4x">PDF</a> <a
                  href="https://github.com/ideo-benchmark-challenges/IDEO">Project Page</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/TAVA.gif" border="0">
            </td>
            <td>
              <p> <b>TAVA: Template-free Animatable Volumetric Actors</b>
                <br>Ruilong Li, Julian Tanke, <b>Minh Vo</b>, Michael Zollhoefer, Jurgen Gall, Angjoo Kanazawa,
                Christoph Lassner<br>
                <em>ECCV 2022 </em> &nbsp; <b></b>
                <br>
                <a href="https://arxiv.org/pdf/2206.08929.pdf">PDF</a> <a
                  href="https://www.liruilong.cn/projects/tava/">Project Page</a>
              </p>
            </td>
          </tr>


          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/lisa_gif.gif" border="0">
            </td>
            <td>
              <p> <b>LISA: Learning Implicit Shape and Appearance of Hands</b>
                <br>Enric Corona, Tomas Hodan, <b>Minh Vo</b>, Francesc Moreno-Noguer, Chris Sweeney, Richard Newcombe,
                and Lingni Ma<br>
                <em>CVPR 2022</em> &nbsp; <b></b>
                <br>
                <a href="https://arxiv.org/abs/2204.01695">PDF</a> <a
                  href="https://www.iri.upc.edu/people/ecorona/lisa/">Project Page</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/banmo.gif" border="0">
            </td>
            <td>
              <p> <b>BANMo: Building Animatable 3D Neural Models from Many Casual Videos</b>
                <br>Gengshan Yang, <b>Minh Vo</b> Natalia Neverova, Deva Ramanan, Andrea Vedaldi, Hanbyul Joo<br>
                <em>CVPR 2022 </em><b><font color="blue">(Oral) </b> &nbsp; <b></b> Acceptance ratio: 344/8161 = 4.2%
                <br>
                <a href="https://arxiv.org/abs/2112.12761">PDF</a> <a href="https://banmo-www.github.io/">Project
                  Page</a>
              </p>
            </td>
          </tr>


          <tr>
            <td width="40.5%">
              <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/ego4d.png" border="0">
            </td>
            <td>
              <p> <b>Ego4D: Around the World in 3,000 Hours of Egocentric Video</b>
                <br>K. Grauman et al.<br>
                <em>CVPR 2022 </em><b> <font color="blue">(Oral - <font color="blue">Best paper finalist and distingished egocentric papers)</b> <b></b> &nbsp; <b></b>Acceptance ratio: 344/8161 = 4.2%
                <br>
                <a href="https://arxiv.org/abs/2110.07058">PDF</a> <a href="https://ego4d-data.org/">Project Page</a>
              </p>
            </td>
          </tr>


    </tbody>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody>
        <tr>
          <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/sNXpsmYeBGM"
              frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
          </td>
          <td width="75%" valign="top">
            <p>
              <b>ODAM: Object Detection, Association, and Mapping using Posed RGB Video</b></a>
              <br>
              Kejie Li, Daniel DeTone, Steven Chen, <b>Minh Vo</b>, Ian Reid, Hamid Rezatofighi, Chris Sweeney, Julian
              Straub, Richard Newcombe
              <br>
              <em>ICCV 2021</em> <b>
                <font color="blue">(Oral)
              </b> &nbsp; <b> </b> Acceptance ratio: 210/6152 = 3.3%
              <br>
              <a href="https://arxiv.org/abs/2108.10165">PDF</a> <a href="https://minhpvo.github.io/">Project Page</a>
            </p>
          </td>
        </tr>


      </tbody>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/YezExZNiQoM"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
            </td>
            <td width="75%" valign="top">
              <p>
                <b>ContactOpt: Optimizing Contact to Improve Grasps</b></a>
                <br>
                Patrick Grady, Chengcheng Tang, Christopher D. Twigg, <b>Minh Vo</b>, Samarth Brahmbhatt, Charles C.
                Kemp
                <br>
                <em>CVPR 2021</em> <b>
                  <font color="blue">(Oral)
                </b> &nbsp; <b> </b> Acceptance ratio: 210/6152 = 3.3%
                <br>
                <a href="https://arxiv.org/pdf/2104.07267.pdf">PDF</a> <a
                  href="http://www.pgrady.net/contactopt/">Project Page</a>
              </p>
            </td>
          </tr>


        </tbody>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/77QPFETBQ_8"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
              </td>
              <td width="75%" valign="top">
                <p>
                  <b>ANR: Articulated Neural Rendering for Virtual Avatars</b></a>
                  <br>
                  Amit Raj, Julian Tanke, James Hays, <b>Minh Vo</b>, Carsten Stoll, and Christoph Lassner
                  <br>
                  <em>CVPR 2021</em> &nbsp; <b> </b>
                  <br>
                  <a href="https://arxiv.org/pdf/2012.12890.pdf">PDF</a> <a
                    href="https://anr-avatars.github.io/">Project Page</a>
                </p>
              </td>
            </tr>

          </tbody>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/uC-AajpAtJw"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                </td>
                <td width="75%" valign="top">
                  <p>
                    <b>TexMesh: Reconstructing Detailed Human Texture and Geometry from Monocular Video</b></a>
                    <br>
                    Tiancheng Zhi, Christoph Lassner, Tony Tung, Carsten Stoll, Srinivasa Narasimhan, and <b>Minh Vo</b>
                    <br>
                    <em>ECCV 2020</em> &nbsp; <b> </b>
                    <br>
                    <a href="./index_files/Papers/ECCV20_Human.pdf">PDF</a> <a
                      href="https://research.fb.com/publications/texmesh-reconstructing-detailed-human-texture-and-geometry-from-rgb-d-video/">Project
                      Page</a>
                  </p>
                </td>
              </tr>

            </tbody>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
                <tr>
                  <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/YfbonliVknA"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                  </td>
                  <td width="75%" valign="top">
                    <p>
                      <b>Long-term Human Motion Prediction with Scene Context</b></a>
                      <br>
                      Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, <b>Minh Vo</b>, and Jitendra Malik
                      <br>
                      <em>ECCV 2020</em> <b>
                        <font color="blue">(Oral) 
                      </b> &nbsp; <b> </b> Acceptance ratio: 104/5025 = 2.0%
                      <br>
                      </a> <a href="./index_files/Papers/ECCV20_MotionScene.pdf">PDF</a> <a
                        href="https://people.eecs.berkeley.edu/~zhecao/hmp/index.html">Project Page</a>
                    </p>
                  </td>
                </tr>


              </tbody>
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/quovnDPwL1k"
                        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                    </td>
                    <td width="75%" valign="top">
                      <p>
                        <b>4D Visualization of Dynamic Events from Unconstrained Multi-View Videos</b></a>
                        <br>
                        Aayush Bansal, <b>Minh Vo</b>, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan
                        <br>
                        <em>CVPR 2020</em> &nbsp; <b> </b>
                        <br>
                        <a href="./index_files/Papers/Open4D_20.pdf">PDF</a> <a
                          href="http://www.cs.cmu.edu/~aayushb/Open4D">Project Page</a>
                        <br> Press Coverage:
                        <a
                          href="https://www.cmu.edu/news/stories/archives/2020/july/smartphone-system-creates-4d-visualizations.html">CMU</a>,
                        <a
                          href="https://cacm.acm.org/news/246062-system-combines-smartphone-videos-to-create-4d-visualizations/fulltext?fbclid=IwAR1xZA-qYBpAm-qA5FunR1dzclP_gz_qYAV5yODYt0UKZMGpwczqvfwBhmM">ACM</a>,
                        <a
                          href="https://techxplore.com/news/2020-07-combines-smartphone-videos-d-visualizations.html">TechXplore</a>,
                        <a
                          href="https://scienmag.com/new-system-combines-smartphone-videos-to-create-4d-visualizations/?fbclid=IwAR3WumDWsTnIG9a0bvy5Ye6XNiQeek9PlQ6Ol1xIwr3I3dIUJ55QSdVDaAw">ScienceMag</a>,
                        and many <a href="./index_files/Images/Press/Open4D.txt">others.</a>
                      </p>
                    </td>
                  </tr>

                </tbody>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody>
                    <tr>
                      <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/4VUo9u-ZLNY"
                          frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                      </td>
                      <td width="75%" valign="top">
                        <p>
                          <b>Spatiotemporal Bundle Adjustment for Dynamic 3D Human Reconstruction in the Wild</b></a>
                          <br> <b>Minh Vo</b>, Srinivasa Narasimhan, and Yaser Sheikh
                          <br>
                          <em>TPAMI 2020 and CVPR 2016</em> &nbsp; <b> </b>
                          <br>
                          <a href="./index_files/Papers/PAMI20_STBA.pdf">PDF</a> <a
                            href="http://www.cs.cmu.edu/~ILIM/projects/IM/STBA/">Project Page</a>
                        </p>
                      </td>
                    </tr>


                  </tbody>
                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                      <tr>
                        <td width="30%"><iframe width="300" height="200" src="https://www.youtube.com/embed/2yspTI3eHGw"
                            frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                        </td>
                        <td width="75%" valign="top">
                          <p>
                            <b> Self-supervised Multi-view Person Association and Its Applications</b>
                            <br>
                            <b>Minh Vo</b>, Ersin Yumer, Kalyan Sunkavalli, Sunil Hadap, Yaser Sheikh, and Srinivasa
                            Narasimhan
                            <br>
                            <em>TPAMI 2020</em> &nbsp; <b> </b>
                            <br>
                            </a> <a href="./index_files/Papers/PAMI20_Asso.pdf">PDF</a> </a> <a
                              href="http://www.cs.cmu.edu/~ILIM/projects/IM/Association4Tracking/">Project Page</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 250px; max-width: 300px;" src="./index_files/Images/ONet_CVPR19.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Occlusion-Net: 2D/3D Occluded Keypoint Localization Using Graph Networks </b>
                            <br>
                            Dinesh Reddy, <b>Minh Vo</b>, and Srinivasa Narasimhan,
                            <br>
                            <em>CVPR 2019 </em> &nbsp; <b> </b>
                            <br>
                            <a href="./index_files/Papers/ONet_19.pdf">PDF</a> <a
                              href="http://www.cs.cmu.edu/~ILIM/projects/IM/CarFusion/cvpr2019/index.html">Project
                              Page</a>
                          </p>
                        </td>
                      </tr>


                      <tr>
                        <td width="30%">
                          <img style="max-height: 250px; max-width: 300px;" src="./index_files/Images/carfusion.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>CarFusion: Combining Part Detection and Point Tracking for Dynamic 3D Reconstruction of
                              Vehicles </b>
                            <br>
                            Dinesh Reddy, <b>Minh Vo</b>, and Srinivasa Narasimhan,
                            <br>
                            <em>CVPR 2018 </em> &nbsp; <b> </b>
                            <br>
                            <a href="./index_files/Papers/CarFusion.pdf">PDF</a> <a
                              href="http://www.cs.cmu.edu/~ILIM/projects/IM/CarFusion/cvpr2018/index.html">Project
                              Page</a>
                          </p>
                        </td>
                      </tr>


                      <tr>
                        <td width="30%">
                          <img style="max-height: 250px; max-width: 300px;" src="./index_files/Images/PAMI_15.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b> Texture Illumination Separation for Single-shot Structured Light Reconstruction </b>
                            <br>
                            <b>Minh Vo</b>, Srinivasa Narasimhan, and Yaser Sheikh
                            <br>
                            <em>CCD 2014</em> and <em>TPAMI 2015</em> &nbsp; <b> </b>
                            <br>
                            <a
                              href="http://www.cs.cmu.edu/~ILIM/projects/IL/TextIllumSep/papers/VNS_TPAMI15.pdf">PDF</a>
                            <a href="http://www.cs.cmu.edu/~ILIM/projects/IL/TextIllumSep/">Project Page</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 150px; max-width: 250px;" src="./index_files/Images/ECCV_14.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Passive Tomography of Turbulance Strength</b>
                            <br>Marina Alterman, Yoav Schechner, <b>Minh Vo</b>, and Srinivasa Narasimhan<br>
                            <em>ECCV 2014</em> &nbsp; <b></b>
                            <br>
                            <a href="http://www.cs.cmu.edu/~ILIM/projects/IM/PassiveCn2/papers/ASVN-ECCV14.pdf"> PDF</a>
                            <a href="http://www.cs.cmu.edu/~ILIM/projects/IM/PassiveCn2/"> Project Page</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 200px; max-width: 250px;" src="./index_files/Images/Strain_14.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Automated fast initial guess in digital image correlation</b>
                            <br>Zhaoyang Wang, <b>Minh Vo</b>, Hien Kieu, Tongyan Pan<br>
                            <em>Strain 2014</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/Strain_14.pdf"> PDF</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 300px; max-width: 300px;" src="./index_files/Images/OPX_12.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Hyper-accurate flexible calibration technique for fringe-projection-based
                              three-dimensional imaging</b>
                            <br><b>Minh Vo</b>, Zhaoyang Wang, Bing Pan, and Tongyan Pan<br>
                            <em>Optics Express 2012</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/OPX_12.pdf"> PDF</a> <a
                              href="https://www.youtube.com/watch?v=RZOxUDz--gc&list=PL5CBpi6HRTswzZdr00ChdneC8Md4BRrG2">
                              Supplementary videos</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 200px; max-width: 300px;" src="./index_files/Images/BOPX_12.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Three-dimensional phantoms for curvature correction in spatial frequency domain
                              imaging</b>
                            <br>Thu Nguyen, Hanh Le, <b>Minh Vo</b>, Zhaoyang Wang, Long Luu, and Jessica
                            Ramella-Roman<br>
                            <em>Biomedical Optics Express 2012</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/BOPX_12.pdf"> PDF</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 200px; max-width: 250px;" src="./index_files/Images/OE_11.jpg"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Advanced geometric camera calibration for machine vision</b>
                            <br><b>Minh Vo</b>, Zhaoyang Wang, Long Luu, and Jun Ma<br>
                            <em>Optical Engineering 2011</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/OE_11.pdf"> PDF</a> <a href="http://opticist.org/">
                              Software</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 250px; max-width: 300px;" border="0">
                        </td>
                        <td>
                          <p> <b>Accuracy enhancement of digital image correlation with B-spline interpolation</b>
                            <br>Long Luu, Zhaoyang Wang, <b>Minh Vo</b>, Thang Hoang, and Jun Ma<br>
                            <em>Optics Letters 2011</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/OL_11.pdf"> PDF</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 200px; max-width: 300px;" src="./index_files/Images/APL_11.png"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Phase extraction from optical interferograms in presence of intensity nonlinearity and
                              arbitrary phase shifts</b>
                            <br>Thang Hoang, Zhaoyang Wang, <b>Minh Vo</b>, Jun Ma, Long Luu, and Bing Pan<br>
                            <em>Applied Physics Letters 2011</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/APL_11.pdf"> PDF</a>
                          </p>
                        </td>
                      </tr>

                      <tr>
                        <td width="30%">
                          <img style="max-height: 250px; max-width: 300px;" src="./index_files/Images/OL_10.jpg"
                            border="0">
                        </td>
                        <td>
                          <p> <b>Flexible calibration technique for fringe-projection-based three-dimensional
                              imaging</b>
                            <br><b>Minh Vo</b>, Zhaoyang Wang, Thang Hoang, and Dung Nguyen<br>
                            <em>Optics Letters 2010</em> &nbsp; <b></b>
                            <br>
                            <a href="./index_files/Papers/OL_10.pdf"> PDF</a>
                          </p>
                        </td>
                      </tr>

                    </tbody>
                  </table>


                  <h2>Others</h2>
                  <table cellspacing="15">
                    <tbody>

                      <tr>
                        <td width="30%">
                        </td>
                        <td>
                          <p> <b> Exploiting Point Motion, Shape Deformation, and Semantic Priors for Dynamic 3D
                              Reconstruction in the Wild</b>
                            <br>
                            <b>Minh Vo</b>
                            <br>
                            <em>Ph.D. Thesis</em> &nbsp; <b> </b>
                            <br>
                            <a href="https://minhpvo.github.io/Thesis.pdf">PDF</a>
                          </p>
                        </td>
                      </tr>


                      </td>
                      </tr>
                    </tbody>
                  </table>

                  <h2>External Coverage</h2>
                  <a
                    href="https://cacm.acm.org/news/246062-system-combines-smartphone-videos-to-create-4d-visualizations/fulltext?fbclid=IwAR1xZA-qYBpAm-qA5FunR1dzclP_gz_qYAV5yODYt0UKZMGpwczqvfwBhmM"><img
                      src="./index_files/Images/Press/acm.jpeg" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://www.cmu.edu/news/stories/archives/2020/july/smartphone-system-creates-4d-visualizations.html"><img
                      src="./index_files/Images/Press/CMU.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a href="https://techxplore.com/news/2020-07-combines-smartphone-videos-d-visualizations.html"><img
                      src="./index_files/Images/Press/TechXplore.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://www.sciencedaily.com/releases/2020/07/200701134244.htm?fbclid=IwAR2XeDWhGo_0chq15glZV7uh51k_qxv6ZLoUFoy-kI1Nlre6DXKjA86w2r8"><img
                      src="./index_files/Images/Press/sciencedaily.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://scienmag.com/new-system-combines-smartphone-videos-to-create-4d-visualizations/?fbclid=IwAR3WumDWsTnIG9a0bvy5Ye6XNiQeek9PlQ6Ol1xIwr3I3dIUJ55QSdVDaAw"><img
                      src="./index_files/Images/Press/sciencemag.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://eandt.theiet.org/content/articles/2020/07/smartphone-videos-combine-to-create-4d-virtualisation-of-filmed-events/?fbclid=IwAR3WumDWsTnIG9a0bvy5Ye6XNiQeek9PlQ6Ol1xIwr3I3dIUJ55QSdVDaAw"><img
                      src="./index_files/Images/Press/et.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://interestingengineering.com/new-system-combines-iphone-videos-for-accessible-4d-video-editing?fbclid=IwAR3c5PrCOCFDkYqoCN53OZNYQWsB7EYTgA1oy9REHpOp2pIzhYdhHV3vqG0"><img
                      src="./index_files/Images/Press/InterestingEngineering.png" height="50"></a>
                  &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://technews.tw/2020/07/03/new-system-combines-smartphone-videos-to-create-4d-visualizations/?fbclid=IwAR0aS1cUdhT6qetSoudWXGukCyEIbOhmfXPpjvw90MqwTBKTokWGxe1KEBM"><img
                      src="./index_files/Images/Press/technewsTW.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://www.journaldugeek.com/2020/07/03/ia-combine-video-4d/?fbclid=IwAR2VfAnX8GQpmkFiu2mIx0l7ZmkvrVvQ5mZdv8nkDXXW-0XlwO56lvlV6mo"><img
                      src="./index_files/Images/Press/jdg.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;
                  <a
                    href="https://www.zdnet.com/article/3d-capture-with-normal-smartphone-cameras/?fbclid=IwAR3PgLFsQTu2zxcYGFB2miprMjHKHTpib8T4Vi2HCGncrfgBCUqBJxrw0bc"><img
                      src="./index_files/Images/Press/zd.jpg" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;

                  <a href="https://www.scs.cmu.edu/news/two-student-teams-win-qualcomm-innovation-fellowships"><img
                      src="./index_files/Images/Press/CMU.png" height="50"></a> &nbsp;&nbsp;&nbsp;&nbsp;



</body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span
    class="gr__triangle"></span></span>

</html>